{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Projeto: BETA BANK\n",
    "Os clientes do Beta Bank estão saindo: pouco a pouco, escapulindo todo mês. Os banqueiros descobriram que é mais barato manter os clientes existentes do que atrair novos. Precisamos prever se um cliente vai deixar o banco em breve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1  Importando as bibliotecas e os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, roc_auc_score, confusion_matrix, recall_score, precision_recall_curve\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import shapiro\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/datasets/Churn.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2  Analizando os dados¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()\n",
    "print(data.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aparentemente os dados possuem tipo de acordo com o previsto. Entretanto a coluna 'Tenure'apresenta cerca de 9% de valores ausentes. As estatísticas descritivas serão analisadas para perceber qual a melhor maneira de realizar o preenchimento destes valores ausentes. \n",
    "\n",
    "As colunas 'Gender' e 'Geography' estão em formato óbject, mas para a posterior análise, será necessário passar para o formato de variáveis dummy, através da aplicação da codificação one-hot a variável 'Gender'e a variável 'Geography' será transformada em ordinal. Isso porque, o gênero e o local da conta podem ser fatores que expliquem a saída de usuários do banco.\n",
    "\n",
    "As colunas 'HasCrCard', 'IsActiveMember' e 'Exited' já estão no formato int64 com valores binários (0 e 1), elas já são consideradas variáveis dummy. Não é necessário aplicar a codificação one-hot nessas colunas, pois elas já estão no formato apropriado para modelos de machine learning.\n",
    "\n",
    "As colunas 'rownumber' (índice das strings de dados), customerId (identificador exclusivo do cliente) e surname (sobrenome) serão excluídas da base de dados, por não serem possíveis variáveis explicativas para a saída de usuários do banco."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3  Preparando os dados\n",
    "3.1  Transformando a coluna Gender em variável dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['Gender'].unique()) # Visualizando os valores únicos na coluna 'Gender'\n",
    "# Transformando a coluna 'Gender'em Codificação One-Hot (OHE) \n",
    "# Retirando uma das colunas para não haver multicolinearidade entre elas\n",
    "data = pd.get_dummies(data, columns=['Gender'], drop_first=True) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2  Transformando a coluna Geography em ordinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['Geography'].unique()) # Visualizando os valores únicos na coluna 'Geography'\n",
    "\n",
    "# Criando uma instância da classe OrdinalEncoder\n",
    "encoder = OrdinalEncoder()\n",
    "\n",
    "# Ajustando o encoder aos dados de 'Geography'\n",
    "encoder.fit(data[['Geography']])  # Passando apenas a coluna 'Geography'\n",
    "\n",
    "# Transformando 'Geography' em uma representação ordinal\n",
    "data['Geography_ordinal'] = encoder.transform(data[['Geography']])\n",
    "\n",
    "# Removendo a coluna original 'Geography'\n",
    "data.drop(columns=['Geography'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3  Descartando as colunas 'RowNumber', 'CustomerId' e 'Surname'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns=['RowNumber', 'CustomerId', 'Surname'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.4  Divisão dos dados em conjuntos de treinamento, validação e teste\n",
    "\n",
    "Dividir os dados em conjuntos de treinamento, teste e validação é essencial para avaliar adequadamente o desempenho do modelo de machine learning. O conjunto de treinamento é usado para treinar o modelo, o conjunto de teste é usado para avaliar o desempenho do modelo em dados não vistos e o conjunto de validação é usado para ajustar os hiperparâmetros do modelo e evitar o superajuste. Essa divisão garante que o modelo seja avaliado de forma justa e imparcial, fornecendo uma estimativa confiável de seu desempenho em dados reais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definindo features e target\n",
    "features = data.drop(columns=['Exited'])\n",
    "target = data['Exited']\n",
    "\n",
    "\n",
    "# Divisão dos dados\n",
    "features_train_valid, features_test, target_train_valid, target_test = train_test_split(\n",
    "    features, target, test_size=0.2, random_state=42)\n",
    "\n",
    "features_train, features_valid, target_train, target_valid = train_test_split(\n",
    "    features_train_valid, target_train_valid, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.5  Valores ausentes na coluna 'Tenure'\n",
    "\n",
    "Em primeiro lugar, uma análise das estatísticas descritivas para a coluna 'Tenure' será realizada. Bem como uma análise gráfica para verificar a distribuição da variável."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Tenure'].describe()\n",
    "\n",
    "# Plotando o histograma\n",
    "plt.hist(data['Tenure'], bins=20, color='skyblue', edgecolor='black')\n",
    "plt.title('Distribuição de Tenure')\n",
    "plt.xlabel('Tenure')\n",
    "plt.ylabel('Frequência')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Plotando gráfico de densidade\n",
    "sns.displot(data['Tenure'], kde=True, color='blue')\n",
    "plt.title('Distribuição de Tenure')\n",
    "plt.xlabel('Tenure')\n",
    "plt.ylabel('Densidade')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aparentemente, a distribuição não é normalmente distribuída. Sendo assim, abaixo é realizado um teste de normalidade de Shapiro. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_test = data['Tenure'].sample(5000)\n",
    "\n",
    "# Removendo os valores ausentes da coluna 'Tenure'\n",
    "data_to_test = data_to_test.dropna()\n",
    "\n",
    "# Teste de normalidade de Shapiro-Wilk\n",
    "stat, p_value = shapiro(data_to_test)\n",
    "\n",
    "# Imprimindo o resultado do teste\n",
    "print(\"Estatística de teste:\", stat)\n",
    "print(\"Valor p:\", p_value)\n",
    "\n",
    "# Interpretando o resultado do teste\n",
    "alpha = 0.05  # Nível de significância\n",
    "if p_value > alpha:\n",
    "    print(\"Os dados parecem seguir uma distribuição normal (não podemos rejeitar a hipótese nula)\")\n",
    "else:\n",
    "    print(\"Os dados não seguem uma distribuição normal (rejeitamos a hipótese nula)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como a variável Tenure não segue distribuição normal, para a imputação de valores ausentes foi escolhido o método pelo modelo de árvore de decisão."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.5.1  Imputando valores nos dados de treinamento¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando um modelo de árvore de decisão\n",
    "tree_model = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "# Removendo os valores ausentes da coluna 'Tenure' nos dados de treinamento\n",
    "features_train_no_missing = features_train.dropna(subset=['Tenure'])\n",
    "target_train_no_missing = target_train[features_train['Tenure'].notna()]\n",
    "\n",
    "# Separando as features e o target sem valores ausentes\n",
    "X_train = features_train_no_missing.drop('Tenure', axis=1)\n",
    "y_train = target_train_no_missing\n",
    "\n",
    "# Treinando o modelo de árvore de decisão\n",
    "tree_model.fit(X_train, y_train)\n",
    "\n",
    "# Identificando os índices dos valores ausentes nos dados de treinamento\n",
    "missing_indices = features_train[features_train['Tenure'].isnull()].index\n",
    "\n",
    "# Preenchendo os valores ausentes usando o modelo treinado\n",
    "X_missing = features_train.loc[missing_indices].drop('Tenure', axis=1)\n",
    "imputed_values = tree_model.predict(X_missing)\n",
    "\n",
    "# Criando uma cópia dos dados de treinamento para evitar o aviso que apareceu anteriormente\n",
    "features_train_copy = features_train.copy()\n",
    "\n",
    "# Preenchendo os valores ausentes nos dados de treinamento\n",
    "features_train_copy.loc[missing_indices, 'Tenure'] = imputed_values\n",
    "\n",
    "# Atribuindo ao DataFrame original\n",
    "features_train = features_train_copy\n",
    "\n",
    "# Verificando se ainda existem valores ausentes nos dados de treinamento\n",
    "print(features_train['Tenure'].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os valores ausentes da coluna 'Tenure' foram preenchidos com sucesso pelo modelo de árvore de decisão."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.5.2  Imputando valores nos dados de validação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remover os valores ausentes dos dados de validação\n",
    "features_valid_no_missing = features_valid.dropna(subset=['Tenure'])\n",
    "\n",
    "# Usar o modelo treinado para prever os valores ausentes nos dados de validação\n",
    "X_valid_missing = features_valid.loc[features_valid.index.difference(features_valid_no_missing.index)].drop('Tenure', axis=1)\n",
    "imputed_values_valid = tree_model.predict(X_valid_missing)\n",
    "\n",
    "# Criando uma cópia dos dados de validação para evitar o aviso alerta de cópia\n",
    "features_valid_copy = features_valid.copy()\n",
    "\n",
    "# Preencher os valores ausentes nos dados de validação\n",
    "features_valid_copy.loc[X_valid_missing.index, 'Tenure'] = imputed_values_valid\n",
    "\n",
    "# Atribuindo ao DataFrame original\n",
    "features_valid = features_valid_copy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.5.3  Imputando valores nos dados de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remover os valores ausentes dos dados de teste\n",
    "features_test_no_missing = features_test.dropna(subset=['Tenure'])\n",
    "\n",
    "# Usar o modelo treinado para prever os valores ausentes nos dados de teste\n",
    "X_test_missing = features_test.loc[features_test.index.difference(features_test_no_missing.index)].drop('Tenure', axis=1)\n",
    "imputed_values_test = tree_model.predict(X_test_missing)\n",
    "\n",
    "# Criando uma cópia dos dados de teste para evitar o aviso alerta de cópia\n",
    "features_test_copy = features_test.copy()\n",
    "\n",
    "# Preencher os valores ausentes nos dados de teste\n",
    "features_test_copy.loc[X_test_missing.index, 'Tenure'] = imputed_values_test\n",
    "\n",
    "# Atribuindo ao DataFrame original\n",
    "features_test = features_test_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.6  Padronizando as características numéricas\n",
    "\n",
    "\n",
    "A padronização das características numéricas é importante porque muitos algoritmos de machine learning assumem que todas as características têm a mesma escala. A padronização transforma as características para que elas tenham média zero e desvio padrão unitário, o que ajuda a evitar que características com magnitudes muito diferentes dominem o modelo. Isso melhora a estabilidade e a eficiência dos algoritmos de aprendizado, garantindo que o modelo seja treinado de maneira mais consistente e robusta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padronizando as características numéricas\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(features_train)  # Ajuste do scaler aos dados de treinamento\n",
    "features_train_scaled = scaler.transform(features_train)  # Padronizando as características de treinamento\n",
    "features_valid_scaled = scaler.transform(features_valid)  # Padronizando as características de validação\n",
    "features_test_scaled = scaler.transform(features_test)  # Padronizando as características de teste, se necessário\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4  Treinando o modelo\n",
    "\n",
    "4.1  Exame do equilíbrio de classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_distribution = target.value_counts(normalize=True) # Examinando o equilíbrio das classes\n",
    "\n",
    "print(\"Distribuição das classes:\")\n",
    "print(class_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isso significa que a classe 0 (por exemplo, clientes que não deixaram o serviço) representa aproximadamente 79.63% do conjunto de dados, enquanto a classe 1 (clientes que deixaram o serviço) representa aproximadamente 20.37%. Essa é uma distribuição desbalanceada, onde uma classe é significativamente mais prevalente do que a outra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2  Treino do modelo sem levar em conta o desequilíbrio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinando o modelo inicial\n",
    "model = RandomForestClassifier(random_state=42) #LogisticRegression(random_state=42)\n",
    "model.fit(features_train, target_train)\n",
    "\n",
    "# Fazendo previsões nos dados de validação\n",
    "predictions = model.predict(features_valid)\n",
    "\n",
    "# Avaliando o desempenho do modelo\n",
    "accuracy = accuracy_score(target_valid, predictions)\n",
    "precision = precision_score(target_valid, predictions)\n",
    "f1 = f1_score(target_valid, predictions)\n",
    "auc_roc = roc_auc_score(target_valid, model.predict_proba(features_valid)[:, 1])\n",
    "conf_matrix = confusion_matrix(target_valid, predictions)\n",
    "\n",
    "# Imprimindo as métricas de desempenho\n",
    "print(\"Acurácia:\", accuracy)\n",
    "print(\"Precisão:\", precision)\n",
    "print(\"F1-score:\", f1)\n",
    "print(\"AUC-ROC:\", auc_roc)\n",
    "print(\"Matriz de Confusão:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O modelo RandomForestClassifier foi treinado sem levar em conta o desequilíbrio inicial das classes. Os resultados mostram que o modelo possui um desempenho razoável na previsão da saída dos clientes do banco. Com uma acurácia de 0.8635, o modelo classifica corretamente aproximadamente 86.1% das instâncias. A precisão de 0.7734 indica que, das instâncias previstas como saída, cerca de 77.3% são realmente saídas, enquanto o F1-score de 0.5919 é uma média harmônica entre precisão e recall. A área sob a curva ROC (AUC-ROC) de 0.8511 indica que o modelo tem uma boa capacidade de distinguir entre as classes positiva e negativa. A matriz de confusão mostra que o modelo está capturando uma quantidade significativa de verdadeiros positivos, mas também está classificando algumas instâncias negativas erroneamente como positivas. Isso sugere que ainda há espaço para melhorias, especialmente na redução dos falsos positivos e na melhoria do recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5  Melhorando a qualidade do modelo\n",
    "\n",
    "5.1  Ajuste de ponderação de classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando e treinando o modelo com ajuste de ponderação da classe\n",
    "model = RandomForestClassifier(class_weight='balanced', random_state=42)# LogisticRegression(class_weight='balanced', random_state=42)\n",
    "model.fit(features_train, target_train)\n",
    "\n",
    "# Fazendo previsões\n",
    "predictions = model.predict(features_valid)\n",
    "\n",
    "# Calcular métricas\n",
    "accuracy = accuracy_score(target_valid, predictions)\n",
    "precision = precision_score(target_valid, predictions)\n",
    "f1 = f1_score(target_valid, predictions)\n",
    "auc_roc = roc_auc_score(target_valid, predictions)\n",
    "conf_matrix = confusion_matrix(target_valid, predictions)\n",
    "\n",
    "# Imprimindo as métricas de desempenho\n",
    "print(\"Acurácia:\", accuracy)\n",
    "print(\"Precisão:\", precision)\n",
    "print(\"F1-score:\", f1)\n",
    "print(\"AUC-ROC:\", auc_roc)\n",
    "print(\"Matriz de Confusão:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao ajustar a ponderação das classes para lidar com o desequilíbrio, observamos que a acurácia, precisão e F1-score permaneceram praticamente inalteradas em relação ao modelo anterior, indicando que o ajuste de ponderação não teve um impacto significativo nessas métricas. No entanto, a AUC-ROC diminuiu ligeiramente para 0.7139, sugerindo uma capacidade ligeiramente inferior do modelo em distinguir entre as classes positiva e negativa em comparação com o modelo anterior. A matriz de confusão permaneceu semelhante, mostrando que o modelo ainda está classificando algumas instâncias negativas erroneamente como positivas. Isso sugere que o ajuste de ponderação pode não ser a melhor abordagem para melhorar o desempenho do modelo neste caso específico."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.2  Superamostragem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para superamostragem (upsampling)\n",
    "def upsample(features, target, repeat):\n",
    "    features_zeros = features[target == 0]\n",
    "    features_ones = features[target == 1]\n",
    "    target_zeros = target[target == 0]\n",
    "    target_ones = target[target == 1]\n",
    "\n",
    "    features_upsampled = pd.concat([features_zeros] + [features_ones] * repeat)\n",
    "    target_upsampled = pd.concat([target_zeros] + [target_ones] * repeat)\n",
    "\n",
    "    features_upsampled, target_upsampled = shuffle(\n",
    "        features_upsampled, target_upsampled, random_state=42\n",
    "    )\n",
    "\n",
    "    return features_upsampled, target_upsampled\n",
    "\n",
    "# Aplicando superamostragem nos dados de treinamento\n",
    "features_upsampled, target_upsampled = upsample(\n",
    "    features_train, target_train, 10\n",
    ")\n",
    "\n",
    "# Treinando o modelo de Regressão Logística com superamostragem\n",
    "log_reg = RandomForestClassifier(random_state=42) #LogisticRegression(solver='liblinear', random_state=42)\n",
    "log_reg.fit(features_upsampled, target_upsampled)\n",
    "\n",
    "# Fazendo previsões no conjunto de validação\n",
    "predicted_valid = log_reg.predict(features_valid)\n",
    "\n",
    "# Calculando e imprimindo a métrica F1\n",
    "print('F1:', f1_score(target_valid, predicted_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao aplicar a técnica de superamostragem (upsampling) nos dados de treinamento, aumentando a quantidade de instâncias da classe minoritária, observamos que o modelo RandomForestClassifier treinado apresentou um F1-score de aproximadamente 0.589 no conjunto de validação. Isso representa uma melhoria em relação ao modelo anterior que não considerou a superamostragem, indicando que essa técnica ajudou o modelo a melhorar sua capacidade de prever corretamente as instâncias da classe positiva. No entanto, é importante notar que o resultado ainda pode ser aprimorado por meio de outras técnicas de ajuste de modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.3  Subamostragem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample(features, target, fraction):\n",
    "    features_zeros = features[target == 0]\n",
    "    features_ones = features[target == 1]\n",
    "    target_zeros = target[target == 0]\n",
    "    target_ones = target[target == 1]\n",
    "\n",
    "    features_downsampled = pd.concat(\n",
    "        [features_zeros.sample(frac=fraction, random_state=42)]\n",
    "        + [features_ones]\n",
    "    )\n",
    "    target_downsampled = pd.concat(\n",
    "        [target_zeros.sample(frac=fraction, random_state=42)]\n",
    "        + [target_ones]\n",
    "    )\n",
    "\n",
    "    features_downsampled, target_downsampled = shuffle(\n",
    "        features_downsampled, target_downsampled, random_state=42\n",
    "    )\n",
    "\n",
    "    return features_downsampled, target_downsampled\n",
    "\n",
    "\n",
    "features_downsampled, target_downsampled = downsample(\n",
    "    features_train, target_train, 0.1\n",
    ")\n",
    "\n",
    "model = RandomForestClassifier(random_state=42) #LogisticRegression(solver='liblinear', random_state=42)\n",
    "model.fit(features_downsampled, target_downsampled)\n",
    "predicted_valid = model.predict(features_valid)\n",
    "\n",
    "print('F1:', f1_score(target_valid, predicted_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A utilização da técnica de subamostragem (undersampling) resultou em um F1-score de 0.48, o que é menor em comparação com a superamostragem. Isso pode indicar que a subamostragem pode ter reduzido a quantidade de dados disponíveis para o modelo aprender, o que pode ter impactado negativamente na capacidade de generalização do modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.4  Ajuste de limiar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(random_state=42) #LogisticRegression(random_state=42, solver='liblinear')\n",
    "model.fit(features_train, target_train)\n",
    "probabilities_valid = model.predict_proba(features_valid)\n",
    "probabilities_one_valid = probabilities_valid[:, 1]\n",
    "\n",
    "for threshold in np.arange(0.1, 0.4, 0.02):\n",
    "    predicted_valid = probabilities_one_valid > threshold\n",
    "    precision = precision_score(target_valid, predicted_valid)\n",
    "    recall = recall_score(target_valid, predicted_valid)\n",
    "    \n",
    "    print(\n",
    "        'Limiar = {:.2f} | Precisão = {:.3f}, Sensibilidade = {:.3f}'.format(\n",
    "            threshold, precision, recall\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.5  Melhor limiar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probabilidades previstas pelo modelo\n",
    "probabilities_valid = model.predict_proba(features_valid)\n",
    "probabilities_one_valid = probabilities_valid[:, 1]\n",
    "\n",
    "# Defina uma faixa de limiares\n",
    "thresholds = np.arange(0, 1, 0.02)\n",
    "\n",
    "best_f1 = 0\n",
    "best_threshold = 0\n",
    "\n",
    "# Itere sobre os limiares e calcule o F1-score para cada um\n",
    "for threshold in thresholds:\n",
    "    predicted_valid = probabilities_one_valid > threshold\n",
    "    precision = precision_score(target_valid, predicted_valid)\n",
    "    recall = recall_score(target_valid, predicted_valid)\n",
    "    f1 = f1_score(target_valid, predicted_valid)\n",
    "    \n",
    "    # Atualize o melhor F1-score e o melhor limiar\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(\"Limiar ideal:\", best_threshold)\n",
    "print(\"Melhor F1-score:\", best_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O ajuste de limiar é uma etapa importante para maximizar o desempenho do modelo, especialmente quando se trata de problemas de classificação desbalanceados, como no seu caso de prever a saída de clientes de um banco. No entanto, ao analisar os resultados, percebemos que o melhor limiar encontrado foi de 0.38, com um F1-score de 0.6157. Isso significa que esse limiar produz um equilíbrio entre precisão e recall, resultando em um bom desempenho geral do modelo.\n",
    "\n",
    "Ao ajustar o limiar para diferentes valores na faixa de 0.1 a 0.4, observamos que o F1-score aumenta até atingir o pico em 0.3 e, em seguida, começa a diminuir gradualmente. Isso indica que o limiar de 0.3 é o ponto ideal que maximiza o F1-score para o seu problema específico."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6  Teste final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testando diferentes combinações de hiperparâmetros\n",
    "hyperparameters = [\n",
    "    {'n_estimators': 100, 'max_depth': None, 'class_weight': None},\n",
    "    {'n_estimators': 100, 'max_depth': 5, 'class_weight': None},\n",
    "    {'n_estimators': 100, 'max_depth': 10, 'class_weight': None},\n",
    "    {'n_estimators': 100, 'max_depth': None, 'class_weight': 'balanced'},\n",
    "    {'n_estimators': 100, 'max_depth': 5, 'class_weight': 'balanced'},\n",
    "    {'n_estimators': 100, 'max_depth': 10, 'class_weight': 'balanced'},\n",
    "    ]\n",
    "\n",
    "best_f1 = 0\n",
    "best_model = None\n",
    "\n",
    "# Iterando sobre as diferentes combinações de hiperparâmetros\n",
    "for params in hyperparameters:\n",
    "    # Criando e treinando o modelo com a combinação atual de hiperparâmetros\n",
    "    model = RandomForestClassifier(random_state=42, **params)\n",
    "    model.fit(features_train, target_train)\n",
    "    \n",
    "    # Fazendo previsões nos dados de validação\n",
    "    predictions_valid = model.predict(features_valid)\n",
    "    \n",
    "    # Calculando o F1-score\n",
    "    f1 = f1_score(target_valid, predictions_valid)\n",
    "    \n",
    "    # Atualizando o melhor F1-score e o melhor modelo\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_model = model\n",
    "\n",
    "# Avaliando o desempenho do melhor modelo nos dados de teste\n",
    "predictions_test = best_model.predict(features_test)\n",
    "accuracy = accuracy_score(target_test, predictions_test)\n",
    "precision = precision_score(target_test, predictions_test)\n",
    "recall = recall_score(target_test, predictions_test)\n",
    "f1 = f1_score(target_test, predictions_test)\n",
    "auc_roc = roc_auc_score(target_test, best_model.predict_proba(features_test)[:, 1])\n",
    "\n",
    "# Imprimindo as métricas do modelo\n",
    "print(\"Desempenho final do melhor modelo RandomForestClassifier no conjunto de teste:\")\n",
    "print(\"Acurácia:\", accuracy)\n",
    "print(\"Precisão:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1)\n",
    "print(\"AUC-ROC:\", auc_roc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao testar diferentes combinações de hiperparâmetros para o modelo RandomForestClassifier, observamos que a melhor combinação resultou em um F1-score de aproximadamente 0.602 no conjunto de teste. Isso significa que o modelo é capaz de prever corretamente cerca de 60,2% das instâncias positivas, alcançando uma acurácia de 0.8385. A precisão do modelo é de aproximadamente 0.584, o que indica que cerca de 58,4% das instâncias previstas como positivas são realmente positivas. O recall, que mede a proporção de instâncias positivas que foram corretamente identificadas pelo modelo, é de aproximadamente 0.621. Além disso, a área sob a curva ROC (AUC-ROC) é de 0.855, indicando uma boa capacidade do modelo em distinguir entre as classes positiva e negativa. Esses resultados sugerem que o modelo RandomForestClassifier ajustado com a melhor combinação de hiperparâmetros tem um desempenho satisfatório na previsão da saída de clientes do banco."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7  Conclusões"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O projeto de previsão de saída de clientes do banco foi conduzido com o objetivo de desenvolver um modelo capaz de identificar clientes propensos a encerrar sua relação com o banco. Inicialmente, exploramos e pré-processamos os dados, lidando com valores ausentes, codificação de variáveis categóricas e divisão dos dados em conjuntos de treinamento, validação e teste. Em seguida, treinamos vários modelos de classificação, como RandomForestClassifier e LogisticRegression, utilizando diferentes abordagens para lidar com o desequilíbrio de classes, como ajuste de ponderação, superamostragem e seleção de hiperparâmetros.\n",
    "\n",
    "Após avaliar o desempenho de cada modelo com métricas como acurácia, precisão, recall, F1-score e AUC-ROC, identificamos o RandomForestClassifier ajustado com a melhor combinação de hiperparâmetros como nosso modelo final. Este modelo obteve um F1-score de aproximadamente 0.602 no conjunto de teste, o que indica uma capacidade razoável de prever corretamente os clientes que estão propensos a sair do banco. No entanto, ainda há espaço para melhorias, especialmente na precisão e recall do modelo.\n",
    "\n",
    "Em resumo, o projeto demonstrou que é possível construir um modelo de previsão de saída de clientes do banco com base em dados históricos. A implementação bem-sucedida deste modelo pode fornecer insights valiosos para a gestão de relacionamento com os clientes, permitindo a adoção de estratégias proativas para retenção de clientes e mitigação do churn, contribuindo assim para a sustentabilidade e crescimento do banco."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
